<div id="user-content-toc" align="center">
  <ul style="list-style: none;">
    <summary><h1>DCoding Tech</h1></summary>
  </ul>
</div>

## She's Good with Computers

My spouse and I were joking about the title of the post. On several occasions, this statement was uttered by older members of our respective families in reference to people that were knowledgeable about technology. 

When follow-up questions (such as, ‚Äúoh, what does she do? or ‚Äúwhere does she work?‚Äù) are posed, the answer is usually ‚ÄúI don‚Äôt know. She‚Äôs just good with computers.‚Äù

This simple answer speaks to how mysterious technology is in our modern culture. For something as ubiquitous in our daily lives as tech, most people don‚Äôt understand what tech entails.

The tech world is broad with a wide range of roles that cater to different aspects of tech, from infrastructure to development. Here are some common roles within tech:

1. Software Development/Engineering
* Software Developer: Designs, creates, and maintains software applications.
* Web Developer: Focuses on building and maintaining websites and web applications.
* Mobile App Developer: Develops applications for mobile platforms (iOS, Android).
* Game Developer: Designs and creates video games, including coding and graphics.
2. Systems and Network Administration
* Systems Administrator: Manages and maintains computer systems and networks, ensuring they run efficiently.
* Network Administrator: Responsible for the setup, maintenance, and management of networks.
* Database Administrator: Manages and organizes data in databases, ensuring data integrity and security.  
3. Cybersecurity
* Cybersecurity Analyst: Protects systems from cyber threats and vulnerabilities by analyzing and responding to incidents.
* Ethical Hacker (Penetration Tester): Tests systems for vulnerabilities to improve security measures.
* Security Engineer: Designs and implements security systems to protect an organization‚Äôs digital assets.
4. Cloud Computing
* Cloud Architect: Designs and manages cloud computing strategies and systems.
* Cloud Engineer: Implements and maintains cloud services and infrastructure.
* Cloud Security Specialist: Focuses on securing cloud-based infrastructure and applications.
5. IT Support
* Help Desk Technician: Provides technical support to end-users, troubleshooting hardware and software issues.
* IT Support Specialist: Offers more specialized technical support and often deals with complex issues.
* Technical Support Engineer: Provides in-depth technical assistance and may also help with software and hardware implementation.  
6. Data and Analytics
* Data Analyst: Gathers and interprets data to help businesses make data-driven decisions.
* Data Scientist: Uses advanced analytics, algorithms, and machine learning techniques to solve complex data problems.
* Business Intelligence Analyst: Focuses on turning data into insights to help guide business strategies.
7. IT Management and Strategy
* IT Project Manager: Manages and oversees IT projects, ensuring they are completed on time and within budget.
* Chief Information Officer (CIO): Oversees the IT department and helps align technology with business goals.
* IT Consultant: Advises companies on how to best use technology to achieve their business objectives. 
8. DevOps and Automation
* DevOps Engineer: Works on the development and operations side to automate processes, ensuring efficient development, deployment, and monitoring.
* Automation Engineer: Focuses on automating repetitive tasks to improve efficiency in systems and processes.
9. Artificial Intelligence and Machine Learning
* AI/Machine Learning Engineer: Designs algorithms and models that enable machines to learn from data.
* AI Research Scientist: Conducts research to develop new AI technologies and techniques.
10. UX/UI Design
* UX Designer: Focuses on improving the user experience by designing intuitive interfaces.
* UI Designer: Designs the visual elements and layout of a user interface.
  
The previous list focuses on abstract technology‚Äîthings you can‚Äôt see or physically touch, like software or cloud networks. However, there are also many hardware-based roles in tech. Hardware refers to the tangible components of a computer or system‚Äîthings you can physically interact with, such as servers.

Hardware-related roles are just as important in tech, especially as we continue to rely on physical devices for computing, networking, and storage. Here are some key roles involved in building, designing, and maintaining hardware:

1. Hardware Engineer
* Electronics Engineer: Designs and develops the electronic components and circuits that are integral to computer hardware (such as motherboards, processors, and power systems).
* Computer Hardware Engineer: Works on the physical components of computers, including the design, testing, and production of things like processors, memory units, and peripheral devices.
* Embedded Systems Engineer: Focuses on the hardware and software combination in embedded systems (like those in consumer electronics, medical devices, or IoT).
2. Systems Architect
* Hardware Systems Architect: Designs and structures the overall layout and integration of hardware components, such as creating the blueprint for how a computer or network system‚Äôs hardware components will work together. They need to ensure compatibility between different hardware elements.  
3. Network Hardware Engineer
* Network Hardware Engineer: Specializes in designing and building network infrastructure components such as routers, switches, and firewalls. They ensure the physical devices that make up the network work seamlessly and efficiently. 
4. Manufacturing Engineer
* Hardware Manufacturing Engineer: Works in the production process of hardware components and ensures the smooth transition from design to mass production. They deal with assembly lines, quality control, and optimizing manufacturing processes.
5. Field Service Technician
* Field Technician: Specializes in installing, maintaining, and repairing hardware systems in real-world environments. They may be responsible for setting up computers, servers, or networking equipment at client locations.
* Hardware Maintenance Technician: Focuses specifically on diagnosing and repairing hardware failures in systems, devices, and equipment.
6. Test Engineer
* Hardware Test Engineer: Responsible for testing hardware components and systems for defects and ensuring they meet performance, safety, and durability standards. This includes stress testing and quality assurance to make sure everything works as expected.
7. Hardware Designer (PCB Design)
* PCB (Printed Circuit Board) Designer: Designs the physical circuit boards that provide the connections for all the electronic components of a device, including computers, phones, and other hardware. They use CAD tools to design the layouts and ensure proper signal flow.
8. Power Systems Engineer
* Power Supply Engineer: Designs and develops the power systems that supply electricity to hardware devices, ensuring that the power requirements are met while maintaining energy efficiency and safety.
9. Hardware Quality Assurance (QA)
* Hardware QA Engineer: Ensures that hardware components, like memory modules, processors, and other devices, are built to spec and meet reliability standards. They perform tests on prototypes and production hardware.
10. IoT Hardware Engineer
* IoT (Internet of Things) Engineer: Specializes in developing hardware and integrated systems for IoT devices, such as sensors, smart devices, wearables, and connected machines. They focus on creating hardware that works efficiently with software to transmit data.
  
Each of these roles can have specific sub-roles or specialties depending on the organization and its needs. But tech is constantly evolving, so new roles and areas of expertise continue to emerge as technology advances.

So, the next time you hear, ‚ÄúShe‚Äôs good with computers,‚Äù you might want to follow up with, ‚ÄúOh yeah? Does she build servers, fight off cybercriminals, or make AI that‚Äôll one day take over the world?‚Äù

Tech isn‚Äôt just ‚Äúcomputers‚Äù‚Äîit‚Äôs an entire universe of roles, from coding and cybersecurity to designing tiny circuit boards and keeping the cloud from crashing. It‚Äôs a vast, dynamic field filled with countless specialized roles‚Äîeach contributing to the technology we rely on daily. Whether in software, hardware, security, or design, tech professionals shape the digital and physical world around us.

Understanding the diversity of these roles not only demystifies tech but also highlights the many opportunities available in the industry. So, if you or someone you know is interested in tech, there‚Äôs a place for you‚Äîno matter where your skills and passions lie.

Here are a few terms that I should have defined earlier:

**Cloud** ‚Äì A way to store and access data, apps, and services over the internet instead of on your personal computer or phone. This information (the data, apps, and services) are kept in a special storage space on servers (more on this below). So, if you use Google Drive, iCloud, or Netflix, you‚Äôre using the cloud. This definition scratches the surface of what you can do in the cloud.

**Network** ‚Äì A group of computers and devices that are connected so they can share information and resources. 

**Server** ‚Äì A specialized computer or system that provides data, services, or resources to other computers (called clients) over a network. It handles requests and responds with the appropriate information, making it a crucial part of the internet and many business systems ‚Äì akin to a server in a restaurant.

**Software** ‚Äì A set of instructions that tells a computer what to do. It‚Äôs the invisible part of a computer that makes everything work.

## On the 1s and 0s

‚ÄúHey, don‚Äôt you mean ‚Äòon the 1s and 2s‚Äô?‚Äù 

If I were talking about DJing, you‚Äôd be right. But this is a tech blog, so 1s and 0s are the beats that make computing go. Like a DJ spinning tracks on the two turntables (each turntable, numbered 1 and 2, indicates which is which), computers work their magic using only 1s and 0s. But instead of beats and bass drops, 1s and 0s correspond to flipping (or not flipping) tiny electronic switches to create everything from emails to epic gaming experiences. So, let‚Äôs drop the bass (or in this case, the bytes ‚Äì more on this later) and dive into the language of computers‚Äîbinary. 

Binary is a number system that uses only 0s and 1s to represent information. Why only two numbers? Because at the deepest level, computers are just tiny electronic circuits that can only do two things:

- 0 = Off (No electricity flowing ‚Äì think of a light switch turned off)
- 1 = On (Electricity flowing ‚Äì think of light switch turned on)
  
You might ask yourself ‚ÄúHow can only two digits represent all the information computers need?‚Äù. Well, it‚Äôs not just those two digits, but it‚Äôs the combinations of those 0s and 1s that represent all the innumerable things computers can do. 

Ok, we‚Äôre going to take a bit of a detour here.

Let me explain how the decimal system (the numbering system we use every day) contrasts with the binary number. 
The decimal system has ten digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. When we count past 9, we combine these digits to make larger numbers (10, 11, 12, etc.). Each place value represents a power of 10 (1s, 10s, 100s, 1000s, and so on). So, the number 325 means (3 x 100) + (2 x 10) + (5 x 1).

Since binary only uses 0 and 1, when we count past 1, we combine those 0s and 1s to make larger numbers as well. However, each place value represents a power of 2 (1s, 2s, 4s, 8s, 16s, and so on). ‚ÄúPower of 2‚Äù refers to how many times you multiply the number 2 by itself. 

Now that we understand binary works in powers of 2, let‚Äôs break down how place values are calculated:

| Power of 2   | Calculation  | Place Value |
| :--- | :--- | :--- |
| 2‚Å∞ | The exponent is 0, any number raised by 0 is always equal to 1, so 2 x 0 = 1.  | 1 |
| 2¬π | The exponent is 1, which means 2 is used as a factor only once. So, 2 x 1 = 2. | 2 |
| 2¬≤ | The exponent is 2, so 2 x 2 = 4. | 4 |
| 2¬≥ | The exponent is 3, so 2 x 2 x 2 = 8. | 8 |
| 2‚Å¥ | The exponent is 4, so 2 x 2 x 2 x 2 = 16. | 16 |
| 2‚Åµ | The exponent is 5, so 2 x 2 x 2 x 2 x 2 = 32. | 32|
| 2‚Å∂ | The exponent is 6, so 2 x 2 x 2 x 2 x 2 x 2  = 64.| 64 |
| 2‚Å∑ | The exponent is 7, so 2 x 2 x 2 x 2 x 2 x 2 x 2 = 128. | 128 |

Here are some decimal numbers and their binary counterparts:

| Decimal | Binary |
| :--- | :---  |
| 0 | <nobr>00000000</nobr> |
| 1 | <nobr>00000001</nobr> |
| 2 | <nobr>00000010</nobr> |
| 150 | <nobr>10010110</nobr> |
| 225 |<nobr>11100001</nobr> |
| 255 | <nobr>11111111</nobr> |

Each 0 or 1 in a binary number is called a bit. Bits are grouped into units of 8 to form bytes. Using a baking analogy, sugar is an important bit in making a chocolate cake, but you can‚Äôt make a chocolate cake with only one cup of sugar. Other ingredients are required to make a chocolate cake; flour, baking powder, salt, vanilla, cocoa powder, milk, and eggs. 

So, just like you combine those 8 ingredients to make a chocolate cake; bits are combined in groups of 8 to form bytes ‚Äì that‚Äôs why decimals like ‚Äú0‚Äù are written as ‚Äú00000000‚Äù in binary. Each byte represents something to a computer. 

Once you go past 11111111 (aka, decimal number 255), which is the endpoint of the 8-bit system, you move to 9-bit, 16-bit, 32-bit, or larger systems. And that, friends, is a bit more than what this blog post is going to cover in detail. 

So what do binary numbers mean to our computer? 

**Letters and Characters:**
   
Computers don‚Äôt understand letters and characters like we do, so they use special encoding standards (ASCII or Unicode) to represent them in binary. 

ASCII (American Standard Code for Information Interchange) is a character encoding system that maps English letters (A-Z, a-z), digits (0-9), and basic symbols to unique 7-bit binary values:
- ‚ÄúA‚Äù = 01000001 
- ‚ÄúB‚Äù = 01000010 
- ‚Äú5‚Äù = 00110101
  
Since ASCII only supports 128 characters, it‚Äôs limited to basic English text.

Unicode, on the other hand, expands on ASCII to support characters from multiple languages, including Chinese, Arabic, emojis, and special symbols. It uses more bits (such as 16-bit or 32-bit) to accommodate thousands of characters:

- ‚ÄúA‚Äù in Unicode is still 01000001, just like ASCII.
- ‚ÄúÊó•‚Äù (Japanese character for ‚Äúsunlight‚Äù) = 11101010 00000000 01001010
- "üòÑ" (Smiley emoji) = 11110000 10011111 10011000 10010000
  
These varying encodings allow for a balance between the efficient use of computer storage space and the need to represent a comprehensive set of characters.

**Images:**
   
Every image on your screen is made up of pixels. Each pixel has a color, and each color is stored as a binary code. A common format is RGB (Red, Green, Blue), where each color gets its own binary value.

Pure red in binary looks like this:

- Red = 11111111 00000000 00000000 (Max red, No green, No blue)
  
RGB has its own notations of representing colors, so you would most likely see pure red referred to as (255, 0, 0) in 8-bit form or as (#FF0000) in hexadecimal form. While binary is the foundation, computers also use hexadecimal notation for more compact representations of large binary numbers. Hexadecimal is a 16-bit numerical system where numbers ‚Äú0‚Äù‚Äì‚Äù9‚Ä≥ represent values 0 to 9 and letters ‚ÄúA‚Äù‚Äì‚ÄùF‚Äù represent values from 10 to 15.

**Sounds:**
   
Have you ever wondered how music is stored digitally? It‚Äôs all just numbers!

- A microphone records sound waves as analog signals.
- The computer converts them into binary numbers by taking thousands of tiny samples per second.
- Higher sample rates mean clearer sound‚Äîlike switching from an old radio to a high-fidelity DJ setup.
  
CD-quality audio takes 44,100 samples per second, meaning the computer stores 44,100 binary numbers every second to reproduce the original sound.

From your favorite songs to Netflix movies, everything in computing comes down to 1s and 0s, endlessly flipping like a DJ‚Äôs crossfader. So next time you send a text, stream a video, or listen to a beat; remember: under the hood, your device is working its own digital turntables, remixing binary data into something we can see, hear, and enjoy.

Beyond text, images, and sound, binary is also the foundation of programming languages and encryption methods that keep your online data secure. This blog post is just scratching (HA! Get it?) the surface of how binary powers computing. 

Here are some links for more information:

- [Decimal to Binary Converter](https://www.binaryhexconverter.com/decimal-to-binary-converter)
- [8-16-32-64-128 bits](https://study.com/academy/lesson/16-32-64-128-bit-integers.html)
- [What is ASCII](https://www.ascii-code.com/articles/Beginners-Guide-to-ASCII)
- [What is Unicode](https://www.twilio.com/docs/glossary/what-is-unicode)

## I'm a Friend of Sarah Connor

For many of us, when we hear "artificial intelligence" (AI), we might think of sci-fi movies like The Terminator and its famous line, "I'll be back." And honestly, some ways AI might develop can be a little scary. But before we dive into the big "what-ifs," let's break down what AI is, how it works, and how we use it today.

AI is when machines are designed to think and act like humans. They can solve problems, learn from experience, make decisions, and understand language-just like people do. We are familiar with machines solving problems. Calculators, personal computers, smartwatches can solve problems more accurately and faster than the average human can. 

But AI goes beyond simple problem-solving. Instead of just following instructions given by humans, AI can learn from experience and make its own decisions based on that learning. It can also understand and respond to human language. For example:

* Online Shopping: Websites recommend products based on what we've bought or viewed previously.
* Virtual Assistants: Siri, Alexa, and Google Assistant listen to our questions and respond.
* Streaming Services: Netflix and YouTube suggest shows and videos based on what we've watched.
* Self-Driving Cars: AI helps vehicles navigate and make driving decisions.

AI can be categorized as the following:

1. Narrow AI (ANI) / Weak AI:
* Definition: ANI is designed to perform specific tasks or in narrow domains. 
* Examples: Facial recognition, speech recognition, recommendation systems (like those used by Netflix or Amazon), and self-driving cars. 
* Limitations: ANI lacks general intelligence and cannot perform tasks outside its specific training. 
2. General AI (AGI) / Strong AI:
* Definition: AGI is a hypothetical AI that possesses human-level intelligence, capable of understanding, learning, and applying knowledge across a wide range of tasks.
* Current Status: AGI is not yet a reality, but is a goal of AI research.
* Capabilities: AGI would be able to perform any intellectual task that a human being can. 
3. Super AI (ASI):
* Definition: ASI is a hypothetical AI that surpasses human intelligence and capabilities. 
* Current Status: ASI is a future possibility, not a current reality. 
* Capabilities: ASI would have the ability to think, reason, solve problems, make judgments, learn, and communicate on its own, potentially better than humans.

AI works by using algorithms and models that process large amounts of data to recognize patterns, make decisions, and improve performance over time. An algorithm is a step-by-step set of instructions that a computer follows to solve a problem or complete a task. It‚Äôs like a recipe that tells the computer exactly what to do in a specific order. The main components of AI include:

* Machine Learning (ML): Algorithms that allow systems to learn from data and improve without explicit programming.
* Deep Learning: A subset of ML that uses neural networks to process complex data like images, speech, and text. Neural networks mimic how the human brain processes information. It helps computers recognize patterns.
* Natural Language Processing (NLP): Enables machines to understand and generate human language (e.g., chatbots, translation tools).
* Computer Vision: Allows AI to interpret and analyze visual data from the world (e.g., facial recognition, autonomous vehicles).
* Robotics: Integrates AI with mechanical systems for automation (e.g., robotic surgery, warehouse automation).
  
AI is widely used across various industries to improve efficiency, accuracy, and decision-making. Some common applications include:

* Healthcare: AI aids in disease diagnosis, drug discovery, robotic surgeries, and personalized medicine.
* Finance: AI detects fraud, automates trading, and offers personalized financial advice.
* Retail & ECommerce: AI powers recommendation systems, chatbots, and inventory management.
* Transportation: Self-driving cars, traffic management, and predictive maintenance for vehicles.
* Entertainment: AI is used in content recommendation (Netflix, YouTube), music composition, and even movie production.
* Manufacturing: AI-driven robots assist in production lines, predictive maintenance, and quality control.
* Education: AI personalized learning, automates grading, and provides tutoring through chatbots.
* Cybersecurity: AI detects and prevents cyber threats by analyzing patterns of attacks.
  
Now regarding the killer cyborgs‚Ä¶ Just kidding (I hope). The future of AI is uncertain and widely debated, but most discussions focus on possible advancements and challenges. AI could develop in several different ways:

1. Advancements in AI Capabilities
* General AI (AGI): AI reaching human-like intelligence, capable of reasoning, problem-solving, and adapting across multiple domains, just like a human.
* Super AI (ASI): AI surpassing human intelligence in all aspects, potentially leading to unpredictable consequences.
* Self-improving AI: AI that can rewrite its own code to improve autonomously, leading to exponential growth in intelligence.
2. AI‚Äôs Role in Society
* AI-Augmented Humans: Brain-computer interfaces (BCIs) allowing direct communication between humans and AI, enhancing cognitive abilities.
* Fully Automated Societies: AI managing economies, governance, and labor, leading to increased efficiency but raising ethical and social concerns.
* Personalized AI Assistants: AI companions becoming more emotionally intelligent and assisting in mental health, relationships, and education.
3. AI in Scientific and Technological Breakthroughs
* Cure for Diseases: AI accelerating drug discovery, genetic engineering, and even personalized medicine tailored to individual DNA.
* Climate Change Solutions: AI optimizing energy use, predicting natural disasters, and developing new sustainable materials.
* Space Exploration: AI-powered robotics and autonomous systems assisting in interstellar exploration and colonization of planets.
4. Potential Risks & Challenges
* Job Displacement: Mass automation may replace human jobs, requiring a shift in workforce skills or universal basic income.
* Ethical Dilemmas: AI decision-making in warfare, policing, and healthcare could pose moral risks.
* Control & Safety: The existential risk of AI acting beyond human control (e.g., "AI alignment problem" ‚Äì ensuring AI‚Äôs goals align with human values).
* Bias & Discrimination: AI reflecting human biases in decision-making, affecting fairness in hiring, lending, and law enforcement.
5. The AI Singularity
* A controversial theory suggests that once AI reaches a point where it can continuously improve itself, it could lead to an intelligence explosion (the singularity). 
* Some predict this could be beneficial (ending human labor, solving global issues), while others fear it could be dangerous if AI‚Äôs goals diverge from human interests; e.g., the killer cyborgs.

The future of AI is uncertain, but it holds incredible potential and significant risks. The key to a positive AI future lies in ethical development, human oversight, and regulation to ensure AI remains beneficial to society.

This is why I wrote this blog. It‚Äôs up to all of us (whether you are an AI engineer or a layperson) -to keep a close eye on how AI develops. It‚Äôs up to us to question if those developments are ethical to all life (both flora and fauna) in every part of the world. It‚Äôs up to our governments to create (and citizens to demand) appropriate regulations to keep AI safe and positive. Or else, we may create a world that we won‚Äôt like. 







